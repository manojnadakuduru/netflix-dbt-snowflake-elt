# ğŸ¬ Netflix Analytics Engineering Project - DBT

[![dbt](https://img.shields.io/badge/dbt-FF694B?style=for-the-badge&logo=dbt&logoColor=white)](https://www.getdbt.com/)
[![Snowflake](https://img.shields.io/badge/Snowflake-29B5E8?style=for-the-badge&logo=snowflake&logoColor=white)](https://www.snowflake.com/)
[![AWS S3](https://img.shields.io/badge/AWS_S3-569A31?style=for-the-badge&logo=amazon-s3&logoColor=white)](https://aws.amazon.com/s3/)

> A production-grade ELT pipeline demonstrating modern analytics engineering best practices using Snowflake, dbt Core, and AWS S3.

---

## ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [Architecture](#-architecture)
- [Data Modeling Strategy](#-data-modeling-strategy)
- [Key Features](#-key-features)
- [Tech Stack](#-tech-stack)
- [Project Structure](#-project-structure)
- [Setup Instructions](#-setup-instructions)
- [Engineering Concepts](#-engineering-concepts-demonstrated)
- [Contributing](#-contributing)

---

## ğŸ¯ Overview

This project showcases a **production-style analytics engineering workflow** that transforms raw MovieLens data into business-ready analytics models. The pipeline demonstrates why **dbt is powerful for modern ELT-based analytics engineering** by implementing:

- **Layered dimensional modeling** (staging â†’ dimension â†’ fact â†’ mart)
- **Data quality testing** and validation frameworks
- **SCD Type 2** for slowly changing dimensions
- **Incremental processing** for performance optimization
- **Automated documentation** with full lineage tracking
- **Reusable macros** and modular SQL engineering

The end result is a robust, scalable, and well-governed data pipeline that serves analytics and BI tools like Power BI, Tableau, and Looker Studio.

---

## ğŸ— Architecture


![Architecture Preview](https://github.com/manojnadakuduru/netflix-dbt-snowflake-elt/blob/main/Architecture.png)

### Pipeline Flow

1. **Extract & Load**: Raw data uploaded to AWS S3 â†’ loaded into Snowflake raw layer using `COPY INTO`
2. **Transform**: dbt orchestrates multi-layered SQL transformations with dependency management
3. **Serve**: Business-ready models consumed by BI tools for analytics and reporting

---

## ğŸ§± Data Modeling Strategy

### Layer Structure

#### ğŸ”¹ **Staging Layer** (`models/staging/`)

**Purpose**: Standardize and prepare raw data for transformation

- Rename columns to consistent naming conventions
- Cast data types appropriately
- Apply basic data cleaning
- Create structured foundation for downstream models

**Models**:
- `src_movies` - Movie catalog with titles and genres
- `src_ratings` - User ratings and timestamps
- `src_tags` - User-generated movie tags
- `src_links` - External ID mappings (TMDB, IMDB)
- `src_genome_tags` - Tag relevance scores
- `src_genome_score` - Tag-movie relevance data

**Materialization**: Primarily `view` for flexibility and storage efficiency

---

#### ğŸ”¹ **Dimension Layer** (`models/dim/`)

**Purpose**: Create descriptive entities with clean primary keys

**Models**:
- `dim_movies` - Movie dimension with enriched attributes
- `dim_users` - User dimension (derived from ratings)
- `dim_genome_tags` - Tag taxonomy and descriptions
- `dim_movies_with_tags` - Movies with aggregated tag information

**Materialization**: `table` for optimized query performance

---

#### ğŸ”¹ **Fact Layer** (`models/fct/`)

**Purpose**: Capture measurable business events and metrics

**Models**:
- `fct_ratings` - User rating events with foreign keys to dimensions
- `fct_genome_scores` - Tag relevance scores for content analysis

**Materialization**: `table` with `incremental` processing where applicable

---

#### ğŸ”¹ **Mart Layer** (`models/mart/`)

**Purpose**: Business-ready datasets optimized for specific analytical use cases

**Models**:
- `mart_movie_releases` - Comprehensive movie analytics with release dates, ratings, and tags

**Materialization**: `table` or `materialized_view` for BI tool performance

---

## âœ¨ Key Features

### ğŸ¯ Materialization Strategies

This project demonstrates **multiple dbt materializations** to optimize performance and manage warehouse costs:

| Strategy | Use Case | Models |
|----------|----------|--------|
| `view` | Lightweight staging, always fresh | Staging models |
| `table` | Frequently queried dimensions | Dimension & fact tables |
| `incremental` | Large datasets, append-only | Future enhancement for ratings |
| `ephemeral` | CTEs for code reusability | Intermediate transformations |

---

### ğŸ” Data Quality & Governance

#### **Generic Tests** (`schema.yml`)
- `not_null` - Ensure critical fields have values
- `unique` - Validate primary keys
- `relationships` - Enforce referential integrity
- `accepted_values` - Check categorical data

#### **Singular Tests**
- `relevance_score_test.sql` - Custom business logic validation

#### **Macro-Based Validation**
- `no_nulls_in_columns.sql` - Bulk null checking across multiple columns

**Result**: Only validated, high-quality data flows downstream

---

### ğŸ•’ Snapshots (SCD Type 2)

**`snap_tags.sql`** implements Slowly Changing Dimension Type 2 for historical tracking:

```sql
{% snapshot snap_tags %}
    {{
        config(
            target_schema='snapshots',
            unique_key='tag_id',
            strategy='timestamp',
            updated_at='updated_at'
        )
    }}
    SELECT * FROM {{ ref('src_tags') }}
{% endsnapshot %}
```

**Tracks**:
- `dbt_valid_from` - Record start date
- `dbt_valid_to` - Record end date
- `dbt_scd_id` - Unique snapshot identifier

**Use Case**: Track how movie tags evolve over time

---

### ğŸŒ± Seeds

**`seed_movie_release_dates.csv`** provides static reference data for:
- Movie release dates
- Production years
- Distribution information

**Load command**:
```bash
dbt seed
```

---

### ğŸ§  Macros

Custom macros in `/macros` for:
- **Code reusability** - DRY principle across models
- **Dynamic SQL generation** - Jinja templating for flexibility
- **Complex logic encapsulation** - Centralized business rules

**Example**: `generate_schema_name.sql` for dynamic schema management

---

### ğŸ“Š Analyses

**`analyses/movie_analysis.sql`** enables:
- Ad-hoc exploratory queries
- Complex analytical prototypes
- Query testing without creating warehouse objects

---

### ğŸ“š Documentation & Lineage

Generate comprehensive documentation with lineage graphs:

```bash
dbt docs generate
dbt docs serve
```

**Provides**:
- âœ… Model descriptions and metadata
- âœ… Column-level documentation
- âœ… Test documentation and results
- âœ… Full DAG visualization showing dependencies
- âœ… Source freshness checks

---

## ğŸ›  Tech Stack

| Component | Technology | Purpose |
|-----------|------------|---------|
| **Data Source** | Netflix/MovieLens CSV | Sample movie and rating data |
| **Cloud Storage** | AWS S3 | Raw data lake |
| **Data Warehouse** | Snowflake | Cloud data platform |
| **Transformation** | dbt Core 1.9.0 | SQL-based transformations |

---

## ğŸ“ Project Structure

```
netflixdbt/
â”œâ”€â”€ analyses/                  # Ad-hoc analysis queries
â”‚   â””â”€â”€ movie_analysis.sql
â”œâ”€â”€ macros/                    # Reusable SQL functions
â”‚   â”œâ”€â”€ generate_schema_name.sql
â”‚   â””â”€â”€ no_nulls_in_columns.sql
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ staging/              # Raw data standardization
â”‚   â”‚   â”œâ”€â”€ src_movies.sql
â”‚   â”‚   â”œâ”€â”€ src_ratings.sql
â”‚   â”‚   â”œâ”€â”€ src_tags.sql
â”‚   â”‚   â”œâ”€â”€ src_links.sql
â”‚   â”‚   â”œâ”€â”€ src_genome_tags.sql
â”‚   â”‚   â””â”€â”€ src_genome_score.sql
â”‚   â”œâ”€â”€ dim/                  # Dimension tables
â”‚   â”‚   â”œâ”€â”€ dim_movies.sql
â”‚   â”‚   â”œâ”€â”€ dim_users.sql
â”‚   â”‚   â”œâ”€â”€ dim_genome_tags.sql
â”‚   â”‚   â””â”€â”€ dim_movies_with_tags.sql
â”‚   â”œâ”€â”€ fct/                  # Fact tables
â”‚   â”‚   â”œâ”€â”€ fct_ratings.sql
â”‚   â”‚   â””â”€â”€ fct_genome_scores.sql
â”‚   â””â”€â”€ mart/                 # Business marts
â”‚       â””â”€â”€ mart_movie_releases.sql
â”œâ”€â”€ seeds/                    # Static reference data
â”‚   â””â”€â”€ seed_movie_release_dates.csv
â”œâ”€â”€ snapshots/                # SCD Type 2 snapshots
â”‚   â””â”€â”€ snap_tags.sql
â”œâ”€â”€ tests/                    # Singular data tests
â”‚   â””â”€â”€ relevance_score_test.sql
â”œâ”€â”€ dbt_project.yml          # dbt project configuration
â”œâ”€â”€ packages.yml             # dbt package dependencies
â””â”€â”€ README.md               # This file
```

---

## ğŸš€ Setup Instructions

### Prerequisites

- Python 3.8 or higher
- Snowflake account with appropriate permissions
- AWS S3 bucket with raw data (or local CSV files)
- Git installed

---

### Step-by-Step Setup

#### 1ï¸âƒ£ **Create Project Directory**

```bash
cd ~/projects
mkdir netflixdbt
cd netflixdbt
```

#### 2ï¸âƒ£ **Create Virtual Environment**

```bash
# Install virtualenv if not already installed
pip3 install virtualenv

# Create and activate virtual environment
virtualenv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

#### 3ï¸âƒ£ **Install dbt with Snowflake Adapter**

```bash
pip install dbt-snowflake==1.9.0
```

#### 4ï¸âƒ£ **Initialize dbt Project**

```bash
dbt init netflix
```

You'll be prompted to configure:
- **Snowflake account** (e.g., `xy12345.us-east-1`)
- **User** (your Snowflake username)
- **Password** (your Snowflake password)
- **Role** (e.g., `ACCOUNTADMIN`, `SYSADMIN`)
- **Warehouse** (e.g., `COMPUTE_WH`)
- **Database** (e.g., `NETFLIX_DB`)
- **Schema** (e.g., `PUBLIC`)
- **Threads** (number of concurrent models, e.g., `4`)

This creates `~/.dbt/profiles.yml` with your connection details.

#### 5ï¸âƒ£ **Verify Connection**

```bash
cd netflix
dbt debug
```

Expected output: `All checks passed!`

#### 6ï¸âƒ£ **Install dbt Packages**

```bash
dbt deps
```

This installs packages defined in `packages.yml` (e.g., `dbt_utils`).

---

## ğŸ“– Usage

### Running the Pipeline

#### **1. Load Raw Data to Snowflake**

In Snowflake, run:

```sql
-- Create stage pointing to S3
CREATE OR REPLACE STAGE netflix_stage
URL = 's3://your-bucket/netflix-data/'
CREDENTIALS = (AWS_KEY_ID = 'your_key' AWS_SECRET_KEY = 'your_secret');

-- Load data
COPY INTO raw_movies FROM @netflix_stage/movies.csv FILE_FORMAT = (TYPE = 'CSV' SKIP_HEADER = 1);
COPY INTO raw_ratings FROM @netflix_stage/ratings.csv FILE_FORMAT = (TYPE = 'CSV' SKIP_HEADER = 1);
-- Repeat for other tables...
```

#### **2. Load Seed Data**

```bash
dbt seed
```

Loads static reference data from `seeds/`.

#### **3. Run All Models**

```bash
dbt run
```

Executes all transformations in dependency order.

#### **4. Run Specific Model**

```bash
dbt run --select dim_movies
```

#### **5. Run Tests**

```bash
dbt test
```

Validates data quality across all models.

#### **6. Run Snapshots**

```bash
dbt snapshot
```

Creates SCD Type 2 historical records.

#### **7. Generate Documentation**

```bash
dbt docs generate
dbt docs serve
```

Opens documentation site at `http://localhost:8080`.

---


## ğŸ“ Engineering Concepts Demonstrated

This project showcases **production-grade analytics engineering practices**:

### ğŸ† Architecture & Design
- âœ… Modern **ELT architecture** (Extract-Load-Transform)
- âœ… **Layered dimensional modeling** (Kimball methodology)
- âœ… **Separation of concerns** (staging â†’ dim â†’ fact â†’ mart)
- âœ… **Schema-on-read** approach for flexibility

### âš¡ Performance & Optimization
- âœ… Snowflake **warehouse optimization** strategies
- âœ… **Incremental processing** for large datasets
- âœ… **Materialization strategy** selection based on use case
- âœ… **Query performance tuning** through proper modeling

### ğŸ” Data Quality & Governance
- âœ… Comprehensive **data validation framework**
- âœ… **Referential integrity** enforcement
- âœ… **SCD Type 2 implementation** for historical tracking
- âœ… **Source freshness monitoring**
- âœ… **Automated documentation** and lineage

### ğŸ§© Code Quality
- âœ… **Modular SQL engineering** with reusable components
- âœ… **Analytics-ready marts** for business users


## ğŸ“¬ Contact

**https://www.linkedin.com/in/manojnadakuduru/** 

---

