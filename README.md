ğŸ¬ Netflix Analytics Engineering Platform
Modern ELT Pipeline using Snowflake + dbt Core
ğŸ“Œ Executive Summary
This project demonstrates a production-grade Analytics Engineering ELT pipeline built using:
Amazon S3 for raw data storage
Snowflake as the cloud data warehouse
dbt Core as the transformation, testing, and governance layer
Raw Netflix MovieLens-style data is ingested into Snowflake and transformed using a layered dimensional modeling approach (Staging â†’ Dimensions â†’ Facts â†’ Marts).
This project showcases:
Modular ELT architecture
Layered dimensional modeling
Data quality testing framework
Incremental model processing
SCD Type 2 snapshots
Reusable macros
Seeds for static reference data
Automated documentation & lineage
The goal is to demonstrate how modern data teams build scalable, governed, production-ready ELT pipelines using dbt.
ğŸ— Architecture Overview

High-Level Flow
Extract â†’ Load â†’ Transform â†’ Serve
Detailed Flow
Netflix CSV
â†’ Amazon S3 (Raw Storage)
â†’ Snowflake (Raw Layer)
â†’ dbt Core (Staging â†’ Dimensions â†’ Facts â†’ Marts)
â†’ BI Tools (Power BI / Tableau / Looker Studio)
ğŸ”¹ Data Ingestion (Raw Layer)
Data is uploaded into Amazon S3 and loaded into Snowflake using:
CREATE STAGE
COPY INTO
Raw Layer Principles
Raw tables remain immutable
No transformations applied at ingestion
Declared as sources inside dbt
Maintains auditability and traceability
This follows a true ELT pattern where transformation happens inside the warehouse.
ğŸ”¹ Transformation Layer (dbt Core)
All transformations are managed using dbt Core and executed inside Snowflake.
dbt handles:
SQL-based transformations
Model dependency resolution (DAG)
Data quality testing
Documentation generation
Snapshot-based historical tracking
Model materialization strategies
This enables scalable and maintainable transformation workflows.
ğŸ§± Data Modeling Strategy
The project follows a layered dimensional modeling architecture.
ğŸ”¹ Staging Layer (models/staging/)
Purpose
Clean raw data
Rename columns
Standardize data types
Prepare structured inputs
Models
src_movies
src_links
src_genome_score
src_tags
src_genome_tags
src_ratings
Materialized primarily as views.
ğŸ”¹ Dimension Layer (models/dim/)
Purpose
Create descriptive entities
Enforce proper grain
Clean primary keys
Optimize joins
Models
dim_movies
dim_users
dim_genome_tags
dim_movies_with_tags
ğŸ”¹ Fact Layer (models/fct/)
Purpose
Store measurable events
Support aggregations and analytics
Models
fct_ratings
fct_genome_scores
ğŸ”¹ Mart Layer (models/mart/)
Purpose
Business-ready reporting datasets
Optimized for BI tools
Simplified analytical structures
Model
mart_movie_releases
âš™ï¸ Materialization Strategies Demonstrated
This project showcases multiple dbt materializations:
view
table
incremental
ephemeral
materialized view
Why This Matters
Optimizes Snowflake compute usage
Demonstrates warehouse cost awareness
Aligns model performance with data layer purpose
Reflects real-world production tuning
ğŸ” Data Quality & Governance
Generic Tests (schema.yml)
not_null
unique
relationships
Singular Tests
relevance_score_test
Macro-Based Validation
no_nulls_in_columns
These validations ensure:
Referential integrity
Column-level validation
Reliable downstream datasets
Trusted analytics
ğŸ•’ Snapshots (SCD Type 2 Implementation)
File: snap_tags.sql
Implements Slowly Changing Dimension Type 2 using:
dbt_valid_from
dbt_valid_to
Capabilities
Historical record preservation
Change tracking
Point-in-time analytics
ğŸŒ± Seeds
File: seed_movie_release_dates.csv
Used for:
Static reference data
Lookup tables
Controlled enrichment
Loaded using dbt seed.
ğŸ§  Macros
Located in the macros directory.
Used to:
Reduce SQL duplication
Encapsulate complex logic
Generate dynamic SQL via Jinja
Standardize validation patterns
Demonstrates modular and scalable SQL engineering.
ğŸ“Š Analyses
Located in analyses/movie_analysis.sql.
Used for:
Ad-hoc analysis
Complex query experimentation
Prototyping analytical logic
Business query validation
Does not create warehouse objects â€” purely analytical exploration.
ğŸ“š Documentation & Lineage
Documentation is generated using dbt docs.
Provides:
Model documentation
Column-level descriptions
Test documentation
Full DAG lineage graph
Ensures transparency and governance across the transformation pipeline.
ğŸš€ How to Run This Project
Step 1 â€“ Create Project Directory
Create a project folder and open it in your IDE.
Step 2 â€“ Create Virtual Environment
Install and activate a Python virtual environment.
Step 3 â€“ Install dbt Snowflake Adapter
Install dbt-snowflake version 1.9.0.
Step 4 â€“ Configure dbt Profile
Create a dbt profile with:
Snowflake account
Username
Password
Role
Warehouse
Database
Schema
Threads
This creates the profiles configuration file.
Step 5 â€“ Install Dependencies
Run dbt deps to install packages.
Step 6 â€“ Execute Models
Run dbt run to build all models.
Step 7 â€“ Execute Tests
Run dbt test to validate data quality.
Step 8 â€“ Run Snapshots
Run dbt snapshot to implement SCD Type 2 tracking.
Step 9 â€“ Load Seeds
Run dbt seed to load static reference data.
Step 10 â€“ Generate Documentation
Generate documentation and serve it locally to visualize lineage and metadata.
ğŸ† Engineering Concepts Demonstrated
Modern ELT architecture
Snowflake warehouse optimization
Layered dimensional modeling
Incremental model processing
SCD Type 2 implementation
Data validation framework
Modular SQL engineering
Source freshness monitoring
Analytics-ready marts
Governance through documentation
DAG-based transformation orchestration
