# ğŸ¬ Netflix Analytics Engineering Project

[![dbt](https://img.shields.io/badge/dbt-FF694B?style=for-the-badge&logo=dbt&logoColor=white)](https://www.getdbt.com/)
[![Snowflake](https://img.shields.io/badge/Snowflake-29B5E8?style=for-the-badge&logo=snowflake&logoColor=white)](https://www.snowflake.com/)
[![AWS S3](https://img.shields.io/badge/AWS_S3-569A31?style=for-the-badge&logo=amazon-s3&logoColor=white)](https://aws.amazon.com/s3/)

> A production-grade ELT pipeline demonstrating modern analytics engineering best practices using Snowflake, dbt Core, and AWS S3.

---

## ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [Architecture](#-architecture)
- [Data Modeling Strategy](#-data-modeling-strategy)
- [Key Features](#-key-features)
- [Tech Stack](#-tech-stack)
- [Project Structure](#-project-structure)
- [Setup Instructions](#-setup-instructions)
- [Usage](#-usage)
- [Engineering Concepts](#-engineering-concepts-demonstrated)
- [Contributing](#-contributing)
- [License](#-license)

---

## ğŸ¯ Overview

This project showcases a **production-style analytics engineering workflow** that transforms raw MovieLens data into business-ready analytics models. The pipeline demonstrates why **dbt is powerful for modern ELT-based analytics engineering** by implementing:

- **Layered dimensional modeling** (staging â†’ dimension â†’ fact â†’ mart)
- **Data quality testing** and validation frameworks
- **SCD Type 2** for slowly changing dimensions
- **Incremental processing** for performance optimization
- **Automated documentation** with full lineage tracking
- **Reusable macros** and modular SQL engineering

The end result is a robust, scalable, and well-governed data pipeline that serves analytics and BI tools like Power BI, Tableau, and Looker Studio.

---

## ğŸ— Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Netflix   â”‚      â”‚   Amazon S3  â”‚      â”‚  Snowflake  â”‚      â”‚  BI Tools    â”‚
â”‚  CSV Data   â”‚â”€â”€â”€â”€â”€â–¶â”‚  Raw Storage â”‚â”€â”€â”€â”€â”€â–¶â”‚  Data WH    â”‚â”€â”€â”€â”€â”€â–¶â”‚ Power BI     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚ Tableau      â”‚
                                                   â”‚              â”‚ Looker       â”‚
                                                   â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                   â–¼
                                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                            â”‚  dbt Core   â”‚
                                            â”‚ Transform   â”‚
                                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Pipeline Flow

1. **Extract & Load**: Raw data uploaded to AWS S3 â†’ loaded into Snowflake raw layer using `COPY INTO`
2. **Transform**: dbt orchestrates multi-layered SQL transformations with dependency management
3. **Serve**: Business-ready models consumed by BI tools for analytics and reporting

---

## ğŸ§± Data Modeling Strategy

### ğŸ“Š Architecture Diagram

![Data Pipeline Architecture](path/to/your/architecture-diagram.png)

### Layer Structure

#### ğŸ”¹ **Staging Layer** (`models/staging/`)

**Purpose**: Standardize and prepare raw data for transformation

- Rename columns to consistent naming conventions
- Cast data types appropriately
- Apply basic data cleaning
- Create structured foundation for downstream models

**Models**:
- `src_movies` - Movie catalog with titles and genres
- `src_ratings` - User ratings and timestamps
- `src_tags` - User-generated movie tags
- `src_links` - External ID mappings (TMDB, IMDB)
- `src_genome_tags` - Tag relevance scores
- `src_genome_score` - Tag-movie relevance data

**Materialization**: Primarily `view` for flexibility and storage efficiency

---

#### ğŸ”¹ **Dimension Layer** (`models/dim/`)

**Purpose**: Create descriptive entities with clean primary keys

**Models**:
- `dim_movies` - Movie dimension with enriched attributes
- `dim_users` - User dimension (derived from ratings)
- `dim_genome_tags` - Tag taxonomy and descriptions
- `dim_movies_with_tags` - Movies with aggregated tag information

**Materialization**: `table` for optimized query performance

---

#### ğŸ”¹ **Fact Layer** (`models/fct/`)

**Purpose**: Capture measurable business events and metrics

**Models**:
- `fct_ratings` - User rating events with foreign keys to dimensions
- `fct_genome_scores` - Tag relevance scores for content analysis

**Materialization**: `table` with `incremental` processing where applicable

---

#### ğŸ”¹ **Mart Layer** (`models/mart/`)

**Purpose**: Business-ready datasets optimized for specific analytical use cases

**Models**:
- `mart_movie_releases` - Comprehensive movie analytics with release dates, ratings, and tags

**Materialization**: `table` or `materialized_view` for BI tool performance

---

## âœ¨ Key Features

### ğŸ¯ Materialization Strategies

This project demonstrates **multiple dbt materializations** to optimize performance and manage warehouse costs:

| Strategy | Use Case | Models |
|----------|----------|--------|
| `view` | Lightweight staging, always fresh | Staging models |
| `table` | Frequently queried dimensions | Dimension & fact tables |
| `incremental` | Large datasets, append-only | Future enhancement for ratings |
| `ephemeral` | CTEs for code reusability | Intermediate transformations |

---

### ğŸ” Data Quality & Governance

#### **Generic Tests** (`schema.yml`)
- `not_null` - Ensure critical fields have values
- `unique` - Validate primary keys
- `relationships` - Enforce referential integrity
- `accepted_values` - Check categorical data

#### **Singular Tests**
- `relevance_score_test.sql` - Custom business logic validation

#### **Macro-Based Validation**
- `no_nulls_in_columns.sql` - Bulk null checking across multiple columns

**Result**: Only validated, high-quality data flows downstream

---

### ğŸ•’ Snapshots (SCD Type 2)

**`snap_tags.sql`** implements Slowly Changing Dimension Type 2 for historical tracking:

```sql
{% snapshot snap_tags %}
    {{
        config(
            target_schema='snapshots',
            unique_key='tag_id',
            strategy='timestamp',
            updated_at='updated_at'
        )
    }}
    SELECT * FROM {{ ref('src_tags') }}
{% endsnapshot %}
```

**Tracks**:
- `dbt_valid_from` - Record start date
- `dbt_valid_to` - Record end date
- `dbt_scd_id` - Unique snapshot identifier

**Use Case**: Track how movie tags evolve over time

---

### ğŸŒ± Seeds

**`seed_movie_release_dates.csv`** provides static reference data for:
- Movie release dates
- Production years
- Distribution information

**Load command**:
```bash
dbt seed
```

---

### ğŸ§  Macros

Custom macros in `/macros` for:
- **Code reusability** - DRY principle across models
- **Dynamic SQL generation** - Jinja templating for flexibility
- **Complex logic encapsulation** - Centralized business rules

**Example**: `generate_schema_name.sql` for dynamic schema management

---

### ğŸ“Š Analyses

**`analyses/movie_analysis.sql`** enables:
- Ad-hoc exploratory queries
- Complex analytical prototypes
- Query testing without creating warehouse objects

---

### ğŸ“š Documentation & Lineage

Generate comprehensive documentation with lineage graphs:

```bash
dbt docs generate
dbt docs serve
```

**Provides**:
- âœ… Model descriptions and metadata
- âœ… Column-level documentation
- âœ… Test documentation and results
- âœ… Full DAG visualization showing dependencies
- âœ… Source freshness checks

---

## ğŸ›  Tech Stack

| Component | Technology | Purpose |
|-----------|------------|---------|
| **Data Source** | Netflix/MovieLens CSV | Sample movie and rating data |
| **Cloud Storage** | AWS S3 | Raw data lake |
| **Data Warehouse** | Snowflake | Cloud data platform |
| **Transformation** | dbt Core 1.9.0 | SQL-based transformations |
| **Orchestration** | dbt | Dependency management & testing |
| **BI Tools** | Power BI, Tableau, Looker Studio | Data visualization |
| **Version Control** | Git & GitHub | Code management |

---

## ğŸ“ Project Structure

```
netflixdbt/
â”œâ”€â”€ analyses/                  # Ad-hoc analysis queries
â”‚   â””â”€â”€ movie_analysis.sql
â”œâ”€â”€ macros/                    # Reusable SQL functions
â”‚   â”œâ”€â”€ generate_schema_name.sql
â”‚   â””â”€â”€ no_nulls_in_columns.sql
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ staging/              # Raw data standardization
â”‚   â”‚   â”œâ”€â”€ src_movies.sql
â”‚   â”‚   â”œâ”€â”€ src_ratings.sql
â”‚   â”‚   â”œâ”€â”€ src_tags.sql
â”‚   â”‚   â”œâ”€â”€ src_links.sql
â”‚   â”‚   â”œâ”€â”€ src_genome_tags.sql
â”‚   â”‚   â””â”€â”€ src_genome_score.sql
â”‚   â”œâ”€â”€ dim/                  # Dimension tables
â”‚   â”‚   â”œâ”€â”€ dim_movies.sql
â”‚   â”‚   â”œâ”€â”€ dim_users.sql
â”‚   â”‚   â”œâ”€â”€ dim_genome_tags.sql
â”‚   â”‚   â””â”€â”€ dim_movies_with_tags.sql
â”‚   â”œâ”€â”€ fct/                  # Fact tables
â”‚   â”‚   â”œâ”€â”€ fct_ratings.sql
â”‚   â”‚   â””â”€â”€ fct_genome_scores.sql
â”‚   â””â”€â”€ mart/                 # Business marts
â”‚       â””â”€â”€ mart_movie_releases.sql
â”œâ”€â”€ seeds/                    # Static reference data
â”‚   â””â”€â”€ seed_movie_release_dates.csv
â”œâ”€â”€ snapshots/                # SCD Type 2 snapshots
â”‚   â””â”€â”€ snap_tags.sql
â”œâ”€â”€ tests/                    # Singular data tests
â”‚   â””â”€â”€ relevance_score_test.sql
â”œâ”€â”€ dbt_project.yml          # dbt project configuration
â”œâ”€â”€ packages.yml             # dbt package dependencies
â””â”€â”€ README.md               # This file
```

---

## ğŸš€ Setup Instructions

### Prerequisites

- Python 3.8 or higher
- Snowflake account with appropriate permissions
- AWS S3 bucket with raw data (or local CSV files)
- Git installed

---

### Step-by-Step Setup

#### 1ï¸âƒ£ **Create Project Directory**

```bash
cd ~/projects
mkdir netflixdbt
cd netflixdbt
```

#### 2ï¸âƒ£ **Create Virtual Environment**

```bash
# Install virtualenv if not already installed
pip3 install virtualenv

# Create and activate virtual environment
virtualenv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

#### 3ï¸âƒ£ **Install dbt with Snowflake Adapter**

```bash
pip install dbt-snowflake==1.9.0
```

#### 4ï¸âƒ£ **Initialize dbt Project**

```bash
dbt init netflix
```

You'll be prompted to configure:
- **Snowflake account** (e.g., `xy12345.us-east-1`)
- **User** (your Snowflake username)
- **Password** (your Snowflake password)
- **Role** (e.g., `ACCOUNTADMIN`, `SYSADMIN`)
- **Warehouse** (e.g., `COMPUTE_WH`)
- **Database** (e.g., `NETFLIX_DB`)
- **Schema** (e.g., `PUBLIC`)
- **Threads** (number of concurrent models, e.g., `4`)

This creates `~/.dbt/profiles.yml` with your connection details.

#### 5ï¸âƒ£ **Verify Connection**

```bash
cd netflix
dbt debug
```

Expected output: `All checks passed!`

#### 6ï¸âƒ£ **Install dbt Packages**

```bash
dbt deps
```

This installs packages defined in `packages.yml` (e.g., `dbt_utils`).

---

## ğŸ“– Usage

### Running the Pipeline

#### **1. Load Raw Data to Snowflake**

In Snowflake, run:

```sql
-- Create stage pointing to S3
CREATE OR REPLACE STAGE netflix_stage
URL = 's3://your-bucket/netflix-data/'
CREDENTIALS = (AWS_KEY_ID = 'your_key' AWS_SECRET_KEY = 'your_secret');

-- Load data
COPY INTO raw_movies FROM @netflix_stage/movies.csv FILE_FORMAT = (TYPE = 'CSV' SKIP_HEADER = 1);
COPY INTO raw_ratings FROM @netflix_stage/ratings.csv FILE_FORMAT = (TYPE = 'CSV' SKIP_HEADER = 1);
-- Repeat for other tables...
```

#### **2. Load Seed Data**

```bash
dbt seed
```

Loads static reference data from `seeds/`.

#### **3. Run All Models**

```bash
dbt run
```

Executes all transformations in dependency order.

#### **4. Run Specific Model**

```bash
dbt run --select dim_movies
```

#### **5. Run Tests**

```bash
dbt test
```

Validates data quality across all models.

#### **6. Run Snapshots**

```bash
dbt snapshot
```

Creates SCD Type 2 historical records.

#### **7. Generate Documentation**

```bash
dbt docs generate
dbt docs serve
```

Opens documentation site at `http://localhost:8080`.

---

### Common Commands

| Command | Description |
|---------|-------------|
| `dbt run` | Run all models |
| `dbt run --select model_name` | Run specific model |
| `dbt run --select model_name+` | Run model and downstream dependencies |
| `dbt run --select +model_name` | Run model and upstream dependencies |
| `dbt test` | Run all tests |
| `dbt test --select model_name` | Test specific model |
| `dbt snapshot` | Run snapshots |
| `dbt seed` | Load seed files |
| `dbt clean` | Delete compiled files |
| `dbt deps` | Install packages |
| `dbt source freshness` | Check source data freshness |

---

## ğŸ“ Engineering Concepts Demonstrated

This project showcases **production-grade analytics engineering practices**:

### ğŸ† Architecture & Design
- âœ… Modern **ELT architecture** (Extract-Load-Transform)
- âœ… **Layered dimensional modeling** (Kimball methodology)
- âœ… **Separation of concerns** (staging â†’ dim â†’ fact â†’ mart)
- âœ… **Schema-on-read** approach for flexibility

### âš¡ Performance & Optimization
- âœ… Snowflake **warehouse optimization** strategies
- âœ… **Incremental processing** for large datasets
- âœ… **Materialization strategy** selection based on use case
- âœ… **Query performance tuning** through proper modeling

### ğŸ” Data Quality & Governance
- âœ… Comprehensive **data validation framework**
- âœ… **Referential integrity** enforcement
- âœ… **SCD Type 2 implementation** for historical tracking
- âœ… **Source freshness monitoring**
- âœ… **Automated documentation** and lineage

### ğŸ§© Code Quality
- âœ… **Modular SQL engineering** with reusable components
- âœ… **DRY principles** through macros
- âœ… **Version control** best practices
- âœ… **Analytics-ready marts** for business users

### ğŸš€ DataOps
- âœ… **Dependency management** and orchestration
- âœ… **Automated testing** in CI/CD pipeline
- âœ… **Environment management** (dev/staging/prod)
- âœ… **Self-service analytics** enablement

---

## ğŸ¤ Contributing

Contributions are welcome! Please follow these steps:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **MovieLens** for providing the sample dataset
- **dbt Labs** for the amazing transformation framework
- **Snowflake** for the cloud data platform
- The **analytics engineering community** for best practices and inspiration

---

## ğŸ“¬ Contact

**Your Name** - [@yourtwitter](https://twitter.com/yourtwitter) - your.email@example.com

**Project Link**: [https://github.com/yourusername/netflix-analytics-engineering](https://github.com/yourusername/netflix-analytics-engineering)

---

<div align="center">

**â­ If you found this project helpful, please consider giving it a star!**

Made with â¤ï¸ by [Your Name]

</div>
